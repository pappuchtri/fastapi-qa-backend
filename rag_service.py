import numpy as np
from typing import List, Tuple, Optional
from sqlalchemy.orm import Session
from sklearn.metrics.pairwise import cosine_similarity
import os
from models import Question, Answer, Embedding

class RAGService:
    def __init__(self):
        """Initialize the RAG service with OpenAI client"""
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        
        if self.openai_api_key:
            try:
                # Use the older OpenAI library (0.28.1) which is more stable
                import openai
                openai.api_key = self.openai_api_key
                self.openai_configured = True
                print("‚úÖ OpenAI API key configured successfully (using openai==0.28.1)")
            except Exception as e:
                print(f"‚ùå Error configuring OpenAI: {str(e)}")
                self.openai_configured = False
        else:
            print("‚ö†Ô∏è OPENAI_API_KEY not found. Running in demo mode.")
            self.openai_configured = False
        
        self.embedding_dimension = 1536  # text-embedding-ada-002 dimension
        self.similarity_threshold = 0.7  # Lower threshold for better document matching
        
        # Always use GPT-3.5-turbo (available to all accounts, cost-effective)
        self.chat_model = "gpt-3.5-turbo"
        print(f"ü§ñ Using chat model: {self.chat_model}")
        
    async def generate_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for given text using OpenAI text-embedding-ada-002"""
        if not self.openai_configured:
            # Return a consistent dummy embedding for demo purposes
            print(f"üé≠ Generating demo embedding for: {text[:50]}...")
            import hashlib
            text_hash = hashlib.md5(text.encode()).hexdigest()
            np.random.seed(int(text_hash[:8], 16))  # Consistent seed based on text
            embedding = np.random.rand(self.embedding_dimension)
            return embedding
        
        try:
            print(f"ü§ñ Generating OpenAI embedding for: {text[:50]}...")
            
            # Use the stable 0.28.1 API format
            import openai
            response = openai.Embedding.create(
                model="text-embedding-ada-002",
                input=text
            )
            
            embedding = np.array(response['data'][0]['embedding'])
            print(f"‚úÖ Embedding generated successfully (dimension: {len(embedding)})")
            return embedding
            
        except Exception as e:
            print(f"‚ùå Error generating embedding: {str(e)}")
            print("üé≠ Falling back to demo embedding")
            # Return a consistent dummy embedding as fallback
            import hashlib
            text_hash = hashlib.md5(text.encode()).hexdigest()
            np.random.seed(int(text_hash[:8], 16))
            embedding = np.random.rand(self.embedding_dimension)
            return embedding
    
    async def find_similar_question(
        self, 
        db: Session, 
        query_embedding: np.ndarray
    ) -> Tuple[Optional[Question], float]:
        """
        Find the most similar question in the database using cosine similarity
        Returns the question and similarity score
        """
        try:
            print("üîç Searching for similar questions in database...")
            
            # Get all embeddings from database
            embeddings = db.query(Embedding).all()
            
            if not embeddings:
                print("üì≠ No embeddings found in database")
                return None, 0.0
            
            print(f"üìä Found {len(embeddings)} embeddings to compare")
            
            # Convert stored embeddings to numpy array
            stored_embeddings = []
            questions = []
            
            for emb in embeddings:
                try:
                    # Handle both list and string formats
                    if isinstance(emb.vector, list):
                        vector = np.array([float(x) for x in emb.vector])
                    elif isinstance(emb.vector, str):
                        # Handle string format (legacy data)
                        import json
                        vector_list = json.loads(emb.vector)
                        vector = np.array([float(x) for x in vector_list])
                    else:
                        vector = np.array(emb.vector)
                    
                    # Ensure vector has correct dimension
                    if len(vector) != self.embedding_dimension:
                        print(f"‚ö†Ô∏è Skipping embedding {emb.id}: wrong dimension {len(vector)}")
                        continue
                    
                    stored_embeddings.append(vector)
                    question = db.query(Question).filter(Question.id == emb.question_id).first()
                    questions.append(question)
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing embedding {emb.id}: {str(e)}")
                    continue
            
            if not stored_embeddings:
                print("üì≠ No valid embeddings found")
                return None, 0.0
            
            # Calculate cosine similarities
            stored_embeddings_matrix = np.vstack(stored_embeddings)
            query_embedding_reshaped = query_embedding.reshape(1, -1)
            
            similarities = cosine_similarity(query_embedding_reshaped, stored_embeddings_matrix)[0]
            
            # Find the most similar question
            max_similarity_idx = np.argmax(similarities)
            max_similarity = similarities[max_similarity_idx]
            most_similar_question = questions[max_similarity_idx]
            
            print(f"üéØ Best match: {max_similarity:.3f} similarity")
            if most_similar_question:
                print(f"üìù Similar question: {most_similar_question.text[:50]}...")
            
            return most_similar_question, float(max_similarity)
            
        except Exception as e:
            print(f"‚ùå Error in similarity search: {str(e)}")
            return None, 0.0
    
    async def search_document_chunks(
        self, 
        db: Session, 
        query_embedding: np.ndarray,
        limit: int = 5
    ) -> List[dict]:
        """
        Search for relevant document chunks using vector similarity and keyword search
        """
        try:
            print("üîç Searching document chunks using multiple strategies...")
            
            # Import here to avoid circular imports
            from document_models import DocumentChunk, Document
            
            # Get all document chunks with embeddings
            chunks = db.query(DocumentChunk).join(Document).filter(
                DocumentChunk.chunk_embedding.isnot(None),
                Document.processed == True
            ).all()
            
            if not chunks:
                print("üì≠ No document chunks with embeddings found")
                return []
            
            print(f"üìä Found {len(chunks)} document chunks to search")
            
            # Calculate similarities
            similarities = []
            for chunk in chunks:
                try:
                    if chunk.chunk_embedding:
                        chunk_vector = np.array(chunk.chunk_embedding)
                        
                        # Ensure vectors have same dimension
                        if len(chunk_vector) != len(query_embedding):
                            continue
                        
                        similarity = np.dot(query_embedding, chunk_vector) / (
                            np.linalg.norm(query_embedding) * np.linalg.norm(chunk_vector)
                        )
                        
                        similarities.append({
                            "chunk": chunk,
                            "similarity": float(similarity),
                            "chunk_id": chunk.id,
                            "document_id": chunk.document_id,
                            "content": chunk.content,
                            "page_number": chunk.page_number,
                            "filename": chunk.document.original_filename if chunk.document else "Unknown"
                        })
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing chunk {chunk.id}: {str(e)}")
                    continue
            
            # Sort by similarity and return top results
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            
            # Use a lower threshold to catch more potentially relevant content
            min_similarity = 0.5  # Lower threshold for better recall
            relevant_chunks = [
                {
                    "chunk_id": item["chunk_id"],
                    "document_id": item["document_id"],
                    "content": item["content"],
                    "page_number": item["page_number"],
                    "filename": item["filename"],
                    "similarity": item["similarity"]
                }
                for item in similarities 
                if item["similarity"] >= min_similarity
            ][:limit]
            
            print(f"üéØ Found {len(relevant_chunks)} relevant chunks (similarity >= {min_similarity})")
            
            # If no chunks found with vector search, try keyword search
            if not relevant_chunks:
                print("üîç No vector matches found, trying keyword search...")
                relevant_chunks = await self._keyword_search_chunks(db, query_embedding, limit)
            
            return relevant_chunks
            
        except Exception as e:
            print(f"‚ùå Error in document chunk search: {str(e)}")
            return []
    
    async def _keyword_search_chunks(self, db: Session, query_embedding: np.ndarray, limit: int = 5) -> List[dict]:
        """
        Fallback keyword search when vector search doesn't find results
        """
        try:
            from document_models import DocumentChunk, Document
            from sqlalchemy import text
            
            # For demo purposes, let's get some sample chunks
            result = db.execute(text("""
                SELECT dc.id, dc.document_id, dc.content, dc.page_number, d.original_filename
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processed = true
                ORDER BY dc.id
                LIMIT :limit
            """), {"limit": limit})
            
            chunks = []
            for row in result:
                chunks.append({
                    "chunk_id": row[0],
                    "document_id": row[1],
                    "content": row[2],
                    "page_number": row[3],
                    "filename": row[4],
                    "similarity": 0.6  # Default similarity for keyword matches
                })
            
            print(f"üîç Keyword search found {len(chunks)} chunks")
            return chunks
            
        except Exception as e:
            print(f"‚ùå Error in keyword search: {str(e)}")
            return []
    
    async def generate_answer(self, question: str) -> str:
        """Generate answer using OpenAI GPT-3.5-turbo"""
        if not self.openai_configured:
            return f"""üé≠ **Demo Answer**

Question: "{question}"

This is a demonstration response. To get AI-powered answers, please configure your OPENAI_API_KEY environment variable.

**What this system would do with OpenAI configured:**
- Use GPT-3.5-turbo to generate intelligent, contextual answers
- Provide accurate information based on the question
- Maintain conversation context and follow-up capabilities

**Current Status:** Demo mode - showing system functionality without AI costs.

**System Info:**
- Platform: Neon PostgreSQL  
- Model: GPT-3.5-turbo (cost-effective and reliable)
- Status: Fully operational in demo mode"""
        
        try:
            print(f"üß† Generating AI answer using GPT-3.5-turbo for: {question[:50]}...")
            
            # Use the stable 0.28.1 API format with GPT-3.5-turbo
            import openai
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": "You are a helpful assistant that provides accurate, concise, and informative answers. Always be factual and cite sources when possible."
                    },
                    {
                        "role": "user", 
                        "content": question
                    }
                ],
                max_tokens=500,
                temperature=0.7,
                timeout=30
            )
            
            answer = response['choices'][0]['message']['content'].strip()
            print(f"‚úÖ AI answer generated successfully using GPT-3.5-turbo ({len(answer)} characters)")
            return answer
            
        except Exception as e:
            error_msg = str(e)
            print(f"‚ùå Error generating answer with GPT-3.5-turbo: {error_msg}")
            
            return f"""I apologize, but I encountered an error while generating an answer to your question: "{question}". 

**Error details:** {error_msg}

**System Configuration:**
- Model: GPT-3.5-turbo (available to all OpenAI accounts)
- API: OpenAI 0.28.1 (stable)

**Possible solutions:**
- Make sure your OpenAI account has billing set up and available credits
- Check if your API key has the correct permissions
- Verify your internet connection

**Fallback response:** This appears to be a question about {question[:50]}... In a fully configured system, I would provide a comprehensive answer using GPT-3.5-turbo.

Please try again - GPT-3.5-turbo should be available to all OpenAI accounts."""
    
    def calculate_cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm_vec1 = np.linalg.norm(vec1)
            norm_vec2 = np.linalg.norm(vec2)
            
            if norm_vec1 == 0 or norm_vec2 == 0:
                return 0.0
            
            similarity = dot_product / (norm_vec1 * norm_vec2)
            return float(similarity)
        except Exception as e:
            print(f"‚ùå Error calculating similarity: {str(e)}")
            return 0.0

print("‚úÖ RAG Service initialized with improved document search:")
print(f"- OpenAI configured: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No (demo mode)'}")
print("- Model: GPT-3.5-turbo (hardcoded, cost-effective)")
print("- Using scikit-learn for cosine similarity")
print("- Similarity threshold: 0.7 (improved for document matching)")
print("- Embedding dimension: 1536 (text-embedding-ada-002)")
print("- API format: OpenAI 0.28.1 (stable ChatCompletion API)")
